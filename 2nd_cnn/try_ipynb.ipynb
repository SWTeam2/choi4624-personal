{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import morphology\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import wide_resnet50_2, resnet18\n",
    "import datasets.mvtec as mvtec\n",
    "\n",
    "\n",
    "# device setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser('PaDiM')\n",
    "    parser.add_argument('--data_path', type=str, default='D:/dataset/mvtec_anomaly_detection')\n",
    "    parser.add_argument('--save_path', type=str, default='./mvtec_result')\n",
    "    parser.add_argument('--arch', type=str, choices=['resnet18', 'wide_resnet50_2'], default='wide_resnet50_2')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    # load model\n",
    "    if args.arch == 'resnet18':\n",
    "        model = resnet18(pretrained=True, progress=True)\n",
    "        t_d = 448\n",
    "        d = 100\n",
    "    elif args.arch == 'wide_resnet50_2':\n",
    "        model = wide_resnet50_2(pretrained=True, progress=True)\n",
    "        t_d = 1792\n",
    "        d = 550\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    random.seed(1024)\n",
    "    torch.manual_seed(1024)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(1024)\n",
    "\n",
    "    idx = torch.tensor(sample(range(0, t_d), d))\n",
    "\n",
    "    # set model's intermediate outputs\n",
    "    outputs = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "\n",
    "    model.layer1[-1].register_forward_hook(hook)\n",
    "    model.layer2[-1].register_forward_hook(hook)\n",
    "    model.layer3[-1].register_forward_hook(hook)\n",
    "\n",
    "    os.makedirs(os.path.join(args.save_path, 'temp_%s' % args.arch), exist_ok=True)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig_img_rocauc = ax[0]\n",
    "    fig_pixel_rocauc = ax[1]\n",
    "\n",
    "    total_roc_auc = []\n",
    "    total_pixel_roc_auc = []\n",
    "\n",
    "    for class_name in mvtec.CLASS_NAMES:\n",
    "\n",
    "        train_dataset = mvtec.MVTecDataset(args.data_path, class_name=class_name, is_train=True)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=32, pin_memory=True)\n",
    "        test_dataset = mvtec.MVTecDataset(args.data_path, class_name=class_name, is_train=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=32, pin_memory=True)\n",
    "\n",
    "        train_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])\n",
    "        test_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])\n",
    "\n",
    "        # extract train set features\n",
    "        train_feature_filepath = os.path.join(args.save_path, 'temp_%s' % args.arch, 'train_%s.pkl' % class_name)\n",
    "        if not os.path.exists(train_feature_filepath):\n",
    "            for (x, _, _) in tqdm(train_dataloader, '| feature extraction | train | %s |' % class_name):\n",
    "                # model prediction\n",
    "                with torch.no_grad():\n",
    "                    _ = model(x.to(device))\n",
    "                # get intermediate layer outputs\n",
    "                for k, v in zip(train_outputs.keys(), outputs):\n",
    "                    train_outputs[k].append(v.cpu().detach())\n",
    "                # initialize hook outputs\n",
    "                outputs = []\n",
    "            for k, v in train_outputs.items():\n",
    "                train_outputs[k] = torch.cat(v, 0)\n",
    "\n",
    "            # Embedding concat\n",
    "            embedding_vectors = train_outputs['layer1']\n",
    "            for layer_name in ['layer2', 'layer3']:\n",
    "                embedding_vectors = embedding_concat(embedding_vectors, train_outputs[layer_name])\n",
    "\n",
    "            # randomly select d dimension\n",
    "            embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "            # calculate multivariate Gaussian distribution\n",
    "            B, C, H, W = embedding_vectors.size()\n",
    "            embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
    "            mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
    "            cov = torch.zeros(C, C, H * W).numpy()\n",
    "            I = np.identity(C)\n",
    "            for i in range(H * W):\n",
    "                # cov[:, :, i] = LedoitWolf().fit(embedding_vectors[:, :, i].numpy()).covariance_\n",
    "                cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
    "            # save learned distribution\n",
    "            train_outputs = [mean, cov]\n",
    "            with open(train_feature_filepath, 'wb') as f:\n",
    "                pickle.dump(train_outputs, f)\n",
    "        else:\n",
    "            print('load train set feature from: %s' % train_feature_filepath)\n",
    "            with open(train_feature_filepath, 'rb') as f:\n",
    "                train_outputs = pickle.load(f)\n",
    "\n",
    "        gt_list = []\n",
    "        gt_mask_list = []\n",
    "        test_imgs = []\n",
    "\n",
    "        # extract test set features\n",
    "        for (x, y, mask) in tqdm(test_dataloader, '| feature extraction | test | %s |' % class_name):\n",
    "            test_imgs.extend(x.cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "            gt_mask_list.extend(mask.cpu().detach().numpy())\n",
    "            # model prediction\n",
    "            with torch.no_grad():\n",
    "                _ = model(x.to(device))\n",
    "            # get intermediate layer outputs\n",
    "            for k, v in zip(test_outputs.keys(), outputs):\n",
    "                test_outputs[k].append(v.cpu().detach())\n",
    "            # initialize hook outputs\n",
    "            outputs = []\n",
    "        for k, v in test_outputs.items():\n",
    "            test_outputs[k] = torch.cat(v, 0)\n",
    "        \n",
    "        # Embedding concat\n",
    "        embedding_vectors = test_outputs['layer1']\n",
    "        for layer_name in ['layer2', 'layer3']:\n",
    "            embedding_vectors = embedding_concat(embedding_vectors, test_outputs[layer_name])\n",
    "\n",
    "        # randomly select d dimension\n",
    "        embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "        \n",
    "        # calculate distance matrix\n",
    "        B, C, H, W = embedding_vectors.size()\n",
    "        embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
    "        dist_list = []\n",
    "        for i in range(H * W):\n",
    "            mean = train_outputs[0][:, i]\n",
    "            conv_inv = np.linalg.inv(train_outputs[1][:, :, i])\n",
    "            dist = [mahalanobis(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
    "            dist_list.append(dist)\n",
    "\n",
    "        dist_list = np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
    "\n",
    "        # upsample\n",
    "        dist_list = torch.tensor(dist_list)\n",
    "        score_map = F.interpolate(dist_list.unsqueeze(1), size=x.size(2), mode='bilinear',\n",
    "                                  align_corners=False).squeeze().numpy()\n",
    "        \n",
    "        # apply gaussian smoothing on the score map\n",
    "        for i in range(score_map.shape[0]):\n",
    "            score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "        \n",
    "        # Normalization\n",
    "        max_score = score_map.max()\n",
    "        min_score = score_map.min()\n",
    "        scores = (score_map - min_score) / (max_score - min_score)\n",
    "        \n",
    "        # calculate image-level ROC AUC score\n",
    "        img_scores = scores.reshape(scores.shape[0], -1).max(axis=1)\n",
    "        gt_list = np.asarray(gt_list)\n",
    "        fpr, tpr, _ = roc_curve(gt_list, img_scores)\n",
    "        img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
    "        total_roc_auc.append(img_roc_auc)\n",
    "        print('image ROCAUC: %.3f' % (img_roc_auc))\n",
    "        fig_img_rocauc.plot(fpr, tpr, label='%s img_ROCAUC: %.3f' % (class_name, img_roc_auc))\n",
    "        \n",
    "        # get optimal threshold\n",
    "        gt_mask = np.asarray(gt_mask_list)\n",
    "        precision, recall, thresholds = precision_recall_curve(gt_mask.flatten(), scores.flatten())\n",
    "        a = 2 * precision * recall\n",
    "        b = precision + recall\n",
    "        f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
    "        threshold = thresholds[np.argmax(f1)]\n",
    "\n",
    "        # calculate per-pixel level ROCAUC\n",
    "        fpr, tpr, _ = roc_curve(gt_mask.flatten(), scores.flatten())\n",
    "        per_pixel_rocauc = roc_auc_score(gt_mask.flatten(), scores.flatten())\n",
    "        total_pixel_roc_auc.append(per_pixel_rocauc)\n",
    "        print('pixel ROCAUC: %.3f' % (per_pixel_rocauc))\n",
    "\n",
    "        fig_pixel_rocauc.plot(fpr, tpr, label='%s ROCAUC: %.3f' % (class_name, per_pixel_rocauc))\n",
    "        save_dir = args.save_path + '/' + f'pictures_{args.arch}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plot_fig(test_imgs, scores, gt_mask_list, threshold, save_dir, class_name)\n",
    "\n",
    "    print('Average ROCAUC: %.3f' % np.mean(total_roc_auc))\n",
    "    fig_img_rocauc.title.set_text('Average image ROCAUC: %.3f' % np.mean(total_roc_auc))\n",
    "    fig_img_rocauc.legend(loc=\"lower right\")\n",
    "\n",
    "    print('Average pixel ROCUAC: %.3f' % np.mean(total_pixel_roc_auc))\n",
    "    fig_pixel_rocauc.title.set_text('Average pixel ROCAUC: %.3f' % np.mean(total_pixel_roc_auc))\n",
    "    fig_pixel_rocauc.legend(loc=\"lower right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(args.save_path, 'roc_curve.png'), dpi=100)\n",
    "\n",
    "\n",
    "def plot_fig(test_img, scores, gts, threshold, save_dir, class_name):\n",
    "    num = len(scores)\n",
    "    vmax = scores.max() * 255.\n",
    "    vmin = scores.min() * 255.\n",
    "    for i in range(num):\n",
    "        img = test_img[i]\n",
    "        img = denormalization(img)\n",
    "        gt = gts[i].transpose(1, 2, 0).squeeze()\n",
    "        heat_map = scores[i] * 255\n",
    "        mask = scores[i]\n",
    "        mask[mask > threshold] = 1\n",
    "        mask[mask <= threshold] = 0\n",
    "        kernel = morphology.disk(4)\n",
    "        mask = morphology.opening(mask, kernel)\n",
    "        mask *= 255\n",
    "        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode='thick')\n",
    "        fig_img, ax_img = plt.subplots(1, 5, figsize=(12, 3))\n",
    "        fig_img.subplots_adjust(right=0.9)\n",
    "        norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        for ax_i in ax_img:\n",
    "            ax_i.axes.xaxis.set_visible(False)\n",
    "            ax_i.axes.yaxis.set_visible(False)\n",
    "        ax_img[0].imshow(img)\n",
    "        ax_img[0].title.set_text('Image')\n",
    "        ax_img[1].imshow(gt, cmap='gray')\n",
    "        ax_img[1].title.set_text('GroundTruth')\n",
    "        ax = ax_img[2].imshow(heat_map, cmap='jet', norm=norm)\n",
    "        ax_img[2].imshow(img, cmap='gray', interpolation='none')\n",
    "        ax_img[2].imshow(heat_map, cmap='jet', alpha=0.5, interpolation='none')\n",
    "        ax_img[2].title.set_text('Predicted heat map')\n",
    "        ax_img[3].imshow(mask, cmap='gray')\n",
    "        ax_img[3].title.set_text('Predicted mask')\n",
    "        ax_img[4].imshow(vis_img)\n",
    "        ax_img[4].title.set_text('Segmentation result')\n",
    "        left = 0.92\n",
    "        bottom = 0.15\n",
    "        width = 0.015\n",
    "        height = 1 - 2 * bottom\n",
    "        rect = [left, bottom, width, height]\n",
    "        cbar_ax = fig_img.add_axes(rect)\n",
    "        cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
    "        cb.ax.tick_params(labelsize=8)\n",
    "        font = {\n",
    "            'family': 'serif',\n",
    "            'color': 'black',\n",
    "            'weight': 'normal',\n",
    "            'size': 8,\n",
    "        }\n",
    "        cb.set_label('Anomaly Score', fontdict=font)\n",
    "\n",
    "        fig_img.savefig(os.path.join(save_dir, class_name + '_{}'.format(i)), dpi=100)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def denormalization(x):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    x = (((x.transpose(1, 2, 0) * std) + mean) * 255.).astype(np.uint8)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def embedding_concat(x, y):\n",
    "    B, C1, H1, W1 = x.size()\n",
    "    _, C2, H2, W2 = y.size()\n",
    "    s = int(H1 / H2)\n",
    "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
    "    x = x.view(B, C1, -1, H2, W2)\n",
    "    z = torch.zeros(B, C1 + C2, x.size(2), H2, W2)\n",
    "    for i in range(x.size(2)):\n",
    "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), 1)\n",
    "    z = z.view(B, -1, H2 * W2)\n",
    "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
